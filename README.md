# Neural-Navi

**Multimodal Machine Learning Approach for Real-Time Critical Driving Situation Recognition in Preventive Driver Assistance Systems**

Neural-Navi is a research project that combines camera data and vehicle telemetry to detect critical driving situations in real-time. The system predicts braking and coasting events 1-5 seconds in advance while considering realistic hardware constraints for automotive deployment.

## üèÜ Key Research Findings

### Scientific Contributions

**üî¨ OBD-II Reverse Engineering**
- Successfully extracted proprietary brake signals via Mode 22 command discovery (`b"223F9F"`)
- Binary brake state extraction with physical pedal validation as ground truth

**üìä Extreme Imbalance Handling**  
- Focal loss configuration addresses 1:36 brake event ratio (2.8% positive rate)
- brake_1s: PR-AUC = 0.203 (7.25√ó improvement over random baseline 0.028)
- coast_1s: PR-AUC = 0.740 with 1:14 ratio (7.1% positive rate)

**üèóÔ∏è Modular Architecture Evaluation**
- Systematic comparison of 12 Encoder/Fusion/Decoder combinations
- **Transformer > LSTM**: Superior performance, stability, and generalization
- Simple concatenation fusion outperforms complex attention mechanisms in training stability

**‚ö° Hardware-Aware Performance**
- **Pipeline Latency** (Raspberry Pi 5): YOLO 92.3% (123ms) + Multimodal 5.4% (7ms) = 133ms total
- **Mixed Precision**: 29% speedup for Transformer, degradation for LSTM
- **Memory**: <1.4GB total (within Raspberry Pi 5 8GB constraints)

**üß† Temporal Attention Insights**
- **Recency Bias**: 0.36-0.44 attention weight on last 5 positions vs 0.14-0.18 on first 5
- **Position 19 Convergence**: All Transformer layers focus on final timestep
- **Local Context Priority**: Short-term dependencies more relevant than long-range for brake prediction

**üìà Prediction Horizon Analysis**
- **Dramatic Performance Drop**: 88% degradation from 1s to 5s prediction horizons
- **Optimal Horizons**: 1s and 2s provide best accuracy/utility trade-off for preventive systems

---

## üöÄ Quick Start

### Installation

1. **Clone repository:**
   ```bash
   git clone https://github.com/floriankulig/neural-navi.git
   cd neural-navi
   ```

2. **Create virtual environment:**
   ```bash
   python -m venv venv --system-site-packages  # Raspberry Pi OS (keeps Picamera2)
   # or
   python -m venv venv   # Windows/Mac/Linux
   ```

3. **Activate virtual environment:**
   ```bash
   source venv/bin/activate  # Linux/Mac
   # or
   .\venv\Scripts\activate   # Windows
   ```

4. **Install dependencies:**
   ```bash
   pip install -e .
   ```

### Basic Usage

#### Record driving data:
```bash
# Using Makefile (recommended)
make record

# Direct execution
python record_drive.py
```

#### Detect vehicles in recordings:
```bash
# Interactive viewer
make detect
# or: python detect_vehicles.py --recordings data/recordings
```

#### Train multimodal models:
```bash
# Train single architecture
make train-single-arch ARCH=simple_concat_transformer

# Evaluate trained models
make evaluate-multimodal
```

## üìÅ Project Structure

```
neural-navi/
‚îú‚îÄ‚îÄ record_drive.py          # üéØ Driving data recording
‚îú‚îÄ‚îÄ detect_vehicles.py       # üîç Vehicle detection in recordings
‚îÇ
‚îú‚îÄ‚îÄ src/                     # üîß Core Application Code
‚îÇ   ‚îú‚îÄ‚îÄ recording/           # Data acquisition (camera, telemetry hardware access)
‚îÇ   ‚îú‚îÄ‚îÄ processing/          # Data preprocessing & feature engineering
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ features/        # Derived features (gear, brake force)
‚îÇ   ‚îú‚îÄ‚îÄ model/               # Neural network architectures
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ encoder.py       # Input encoders (Simple, Attention)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fusion.py        # Fusion modules (Concat, Cross-Attention, Query)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ decoder.py       # Output decoders (LSTM, Transformer)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ factory.py       # Model factory for different configurations
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ loss.py          # Unified focal loss system
‚îÇ   ‚îî‚îÄ‚îÄ utils/               # Utilities (config, device setup, helpers)
‚îÇ
‚îú‚îÄ‚îÄ training/                # üß† Training Pipeline & Experiments
‚îÇ   ‚îú‚îÄ‚îÄ datasets/            # Dataset preparation (Boxy, multimodal)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_loaders.py  # HDF5-based efficient data loading
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ boxy_prep...     # Boxy dataset preprocessing
‚îÇ   ‚îú‚îÄ‚îÄ yolo/                # YOLO training for vehicle detection
‚îÇ   ‚îî‚îÄ‚îÄ multimodal/          # Multimodal model training
‚îÇ       ‚îú‚îÄ‚îÄ prepare_dataset.py    # H5 dataset preparation
‚îÇ       ‚îú‚îÄ‚îÄ train_single.py       # Single architecture training
‚îÇ       ‚îî‚îÄ‚îÄ auto_annotate.py      # YOLO-based annotation
‚îÇ
‚îú‚îÄ‚îÄ evaluation/              # üìä Metrics, visualization & analysis
‚îú‚îÄ‚îÄ tests/                   # üìä Development test scripts (access to ECU, sensor sync, etc.)
‚îú‚îÄ‚îÄ jobs/                    # üñ•Ô∏è SLURM job scripts for cluster computing
‚îî‚îÄ‚îÄ data/                    # üìÅ Datasets, models, recordings
    ‚îî‚îÄ‚îÄ ...                  # see below
```

### üìÅ Data Directory Structure
```
data/                           # üíæ Data storage (gitignored)
‚îú‚îÄ‚îÄ cache/                      # Temporary processing cache
‚îú‚îÄ‚îÄ datasets/                   # Dataset storage
‚îÇ   ‚îú‚îÄ‚îÄ processed/              # Processed datasets ready for training
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ annotations/        # Annotated recording data
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ boxy_yolo_n1/       # Boxy dataset in YOLO format (1 class)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ nuimages_yolo/      # NuImages dataset in YOLO format
‚îÇ   ‚îú‚îÄ‚îÄ raw/                    # Raw dataset files (Boxy, NuImages)
‚îÇ   ‚îú‚îÄ‚îÄ boxy_labels.json        # Boxy dataset labels
‚îÇ   ‚îî‚îÄ‚îÄ dataset.yaml            # YOLO dataset configuration
‚îú‚îÄ‚îÄ models/                     # Trained model checkpoints
‚îÇ   ‚îú‚îÄ‚îÄ yolo/                   # Best YOLO model checkpoints
‚îÇ   ‚îî‚îÄ‚îÄ multimodal/             # Multimodal model checkpoints
‚îî‚îÄ‚îÄ recordings/                 # Raw driving recordings
    ‚îî‚îÄ‚îÄ YYYY-MM-DD_HH-MM-SS/    # Recording sessions (timestamped)
        ‚îú‚îÄ‚îÄ telemetry.csv       # OBD-II data with derived features
        ‚îú‚îÄ‚îÄ future_labels.csv   # Ground truth future labels for multimodal training (once generated)
        ‚îú‚îÄ‚îÄ annotations.csv     # YOLO detection results
        ‚îî‚îÄ‚îÄ *.jpg               # Camera frames
```

## üî¨ Research Context

This project develops a **multimodal Machine Learning approach for real-time critical driving situation detection** as part of automotive AI research.

### Central Research Questions:
- How can camera and vehicle telemetry data be combined for improved predictions?
- What are the computational constraints of real-time processing on embedded hardware?
- How do different neural architectures perform under latency constraints?

### Technical Innovation:
- **Hardware-Aware Design**: Optimized for Raspberry Pi deployment (~133ms total pipeline)
- **Modular Architecture**: Systematically comparable encoder/fusion/decoder combinations
- **Real-World Data**: Custom dataset from German Autobahn driving
- **Synchronized Multimodal Capture**: Camera + OBD-II with <5ms synchronization

For detailed information about specific components, see:
- **[Source Code Documentation](src/README.md)** - Model architectures, configuration system
- **[Training Pipeline](training/README.md)** - Dataset preparation, training workflows
- **[Evaluation Framework](evaluation/README.md)** - Scientific metrics, benchmarks, analysis tools

## üèóÔ∏è Model Architecture

### Proven Working Configurations

**‚ö†Ô∏è Training Status**: Only `simple_concat_*` architectures are stable during training. Custom attention implementations have convergence issues.

```python
# Recommended stable configuration
BEST_CONFIG = {
    "encoder_type": "simple",        # Baseline with independent processing
    "fusion_type": "concat",         # Efficient concatenation-based fusion  
    "decoder_type": "transformer",   # Superior performance vs LSTM
    "embedding_dim": 64,
    "hidden_dim": 128,
    "dropout_prob": 0.15
}
```

### Systematic Architecture Evaluation

**Modular Combinatorics for Ablation Studies:**
```python
# Systematic evaluation of all architecture combinations
architectures = {
    "encoders": ["simple", "attention"],
    "fusion": ["concat", "cross_attention", "query"],
    "decoders": ["lstm", "transformer"]
}
# ‚Üí 2 √ó 3 √ó 2 = 12 architecture variants for comparison

# Create model via Factory Pattern
from src.model.factory import create_model_variant
model = create_model_variant(config)
```

## üìä Dataset & Training Status

### Dataset Statistics
- **Total Sequences**: 42,686 (from 12 recordings, 43,104 frames)
- **Brake Events**: 1,200 sequences (2.8% - extreme imbalance, ratio 1:36)
- **Coast Events**: ~3,050 sequences (7.1% - moderate imbalance, ratio 1:14)
- **Dataset Splits**: 70.4% train / 19.5% val / 10.1% test (recording-based)
- **Sequence Length**: 20 frames (10 seconds at 2Hz)

### Prediction Tasks
```python
PREDICTION_TASKS = [
    "brake_1s",   # Primary safety task (2.8% positive rate)
    "brake_2s",   # Secondary safety task  
    "coast_1s",   # Primary efficiency task (7.1% positive rate)
    "coast_2s"    # Secondary efficiency task
]
```

### Advanced Loss System
- **Focal Loss**: Addresses extreme class imbalance with task-specific Œ±/Œ≥ parameters
- **Task Weighting**: Safety-critical tasks prioritized over efficiency tasks
- **Multi-Task Learning**: Simultaneous prediction across multiple horizons

## üõ†Ô∏è Development & Cluster Computing

### Running SLURM Jobs
```bash
# Prepare Boxy dataset
sbatch jobs/boxy_prepare.slurm

# YOLO training
sbatch jobs/boxy_train.slurm

# YOLO validation
sbatch jobs/val_yolo.slurm

# Multimodal pipeline
sbatch jobs/multimodal_pipeline_full.slurm

# Train specific architecture
sbatch --export=ARCHITECTURE=simple_concat_transformer jobs/multimodal_train_single.slurm

# Evaluate models
sbatch jobs/multimodal_evaluate.slurm
```

### Adding New Features

1. **New Model Architecture**: Add to `src/model/` (see [Source Documentation](src/README.md))
2. **New Data Processing**: Add to `src/processing/`
3. **New Training Script**: Add to `training/` with corresponding SLURM job (see [Training Documentation](training/README.md))
4. **New Evaluation Metrics**: Add to `evaluation/` (see [Evaluation Documentation](evaluation/README.md))

## üìä Performance

### Hardware Requirements
- **Recommended**: Raspberry Pi 5 (8GB RAM)
- **Development**: Modern laptop/desktop with GPU for training

### Latency Breakdown (Raspberry Pi 5 8GB)
```
Component                   Latency       Memory      Details
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Camera Capture             ~2.5ms        ~10MB       PiCamera2 1080p
YOLO Inference (YOLOv12n)  ~123ms        ~960MB      Hardware bottleneck
Telemetry Processing       ~2ms          ~1MB        OBD-II + Features
Multimodal Model           ~7ms          ~430MB      Transformer + Mixed Precision
Decision Making            ~1ms          ~1MB        Logic Layer
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Total Pipeline             ~133ms        ~1.4GB      Real-Time Capable
```

**Key Insights:**
- YOLO dominates pipeline latency (92.3%) - primary optimization target
- Multimodal model contributes <6% to total latency
- Memory requirements well within Raspberry Pi 5 constraints

### Training Performance
- **GPU Memory**: 4-16GB recommended for full dataset
- **Batch Size**: 256 (optimized for Nvidia A100 GPUs)
- **Mixed Precision**: Enabled for faster training

## üéØ Project Status

- ‚úÖ **Data Recording System** (Camera + OBD-II)
- ‚úÖ **YOLO Vehicle Detection** (Finetuned on Boxy dataset)
- ‚úÖ **Feature Engineering** (Gear detection, brake force estimation)
- ‚úÖ **Modular Model Architecture** (Encoder/Fusion/Decoder)
- ‚úÖ **OBD-II Reverse Engineering** (Proprietary brake signal extraction)
- ‚úÖ **Multimodal Training Pipeline** (Full H5-based pipeline with focal loss)
- ‚úÖ **Dataset Preparation** (42,686 sequences from 12 recordings)
- ‚úÖ **Systematic Architecture Evaluation** (Transformer > LSTM validated)
- ‚úÖ **Scientific Evaluation Framework** (PR-AUC focus, hardware benchmarks)
- ‚è≥ **Sampling Strategy (Training)** (Planned)

---

**Note**: This is a research project focused on automotive AI safety systems. It is not intended for production use in vehicles without proper safety validation and certification.